Attacking O
machine B_Com_targets_1
learning I_Com_targets_1
algorithms E_Com_targets_1
by O
using O
pertubations B_Tool_uses_1
of I_Tool_uses_1
the I_Tool_uses_1
inputs E_Tool_uses_1
. O

Boundary B_AP_M_1
Attack: O
Attack O
algorithm O
is O
initialized O
at O
a O
point O
that O
is O
already O
adversarial O
. O

Performing O
a O
random O
walk O
along O
the O
boundary O
between O
adversarial O
and O
non-adversarial O
area O
. O

This O
is O
done O
in O
such O
a O
way O
that O
the O
algorithm O
remains O
in O
the O
adversarial O
area O
and O
the O
distance O
to O
the O
target O
image O
becomes O
smaller O
. O

Thus O
, O
one O
tries O
to O
find O
smaller O
adversarial O
pertubations O
based O
on O
an O
adversarial O
criterion O
( O
e.g. O
misclassification). O
Two O
relevant O
parameters: O
Length O
of O
total O
pertubation O
and O
distance O
to O
original O
input O
. O

Boundary O
Attack O
is O
immune O
to O
defenses O
based O
on O
gradient-masking. O
Boundary O
Attack O
has O
been O
successfully O
used O
in O
real O
world O
image O
recognition O
applications O
. O

The O
adversarial O
examples O
, O
which O
were O
created O
by O
the O
Boundary O
Attack O
, O
had O
such O
small O
pertubations O
, O
that O
a O
differentiation O
from O
the O
original O
was O
hardly O
possible O
and O
still O
wrongly O
classified O
. O

